{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias/.virtualenvs/torch/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from DCGAN import DCGAN\n",
    "from fid import calculate_frechet_distance, compute_statistics_of_path, calculate_frechet_distance\n",
    "from inception import InceptionV3\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of channels in the training images. For color images this is 3\n",
    "num_channels = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "latent_dim = 128\n",
    "\n",
    "# Size of feature maps in generator\n",
    "num_generator_features = 128\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "num_discriminator_features = 64\n",
    "\n",
    "NUM_WORKERS = int(os.cpu_count() - 1)\n",
    "BATCH_SIZE = 6 \n",
    "\n",
    "IMAGE_SIZE = 512\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "dataset_path = \"../data/celeba/img_align_celeba/img_align_celeba/\"\n",
    "\n",
    "dataset_path = \"../data/art_dataset/resized/resized/\"\n",
    "dataset_statistics_path = \"/\".join(dataset_path.split(\"/\")[:-1]) + \"/inception_statistics.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCGAN(\n",
    "    num_channels=num_channels,\n",
    "    latent_dim=latent_dim,\n",
    "    num_generator_features=num_discriminator_features,\n",
    "    num_discriminator_features=num_discriminator_features,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model = InceptionV3().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute statistics for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: batch size is bigger than the data size. Setting batch size to data size\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "batch_size should be a positive integer value, but got batch_size=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/elias/dev/personal/art-gan-project/nehmegan/compute_fid.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/elias/dev/personal/art-gan-project/nehmegan/compute_fid.ipynb#ch0000005?line=0'>1</a>\u001b[0m m1, s1 \u001b[39m=\u001b[39m compute_statistics_of_path(dataset_path, inception_model, BATCH_SIZE, \u001b[39m2048\u001b[39;49m, device, \u001b[39m12\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/elias/dev/personal/art-gan-project/nehmegan/compute_fid.ipynb#ch0000005?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(m1, s1)\n",
      "File \u001b[0;32m~/dev/personal/art-gan-project/nehmegan/fid.py:250\u001b[0m, in \u001b[0;36mcompute_statistics_of_path\u001b[0;34m(path, model, batch_size, dims, device, num_workers)\u001b[0m\n\u001b[1;32m    246\u001b[0m     path \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPath(path)\n\u001b[1;32m    247\u001b[0m     files \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(\n\u001b[1;32m    248\u001b[0m         [file \u001b[39mfor\u001b[39;00m ext \u001b[39min\u001b[39;00m IMAGE_EXTENSIONS \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m path\u001b[39m.\u001b[39mglob(\u001b[39m\"\u001b[39m\u001b[39m*.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(ext))]\n\u001b[1;32m    249\u001b[0m     )\n\u001b[0;32m--> 250\u001b[0m     m, s \u001b[39m=\u001b[39m calculate_activation_statistics(\n\u001b[1;32m    251\u001b[0m         files, model, batch_size, dims, device, num_workers\n\u001b[1;32m    252\u001b[0m     )\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m m, s\n",
      "File \u001b[0;32m~/dev/personal/art-gan-project/nehmegan/fid.py:235\u001b[0m, in \u001b[0;36mcalculate_activation_statistics\u001b[0;34m(files, model, batch_size, dims, device, num_workers)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_activation_statistics\u001b[39m(\n\u001b[1;32m    217\u001b[0m     files, model, batch_size\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, dims\u001b[39m=\u001b[39m\u001b[39m2048\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, num_workers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    218\u001b[0m ):\n\u001b[1;32m    219\u001b[0m     \u001b[39m\"\"\"Calculation of the statistics used by the FID.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m    Params:\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39m    -- files       : List of image files paths\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39m               the inception model.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     act \u001b[39m=\u001b[39m get_file_activations(files, model, batch_size, dims, device, num_workers)\n\u001b[1;32m    236\u001b[0m     mu \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(act, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    237\u001b[0m     sigma \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcov(act, rowvar\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/dev/personal/art-gan-project/nehmegan/fid.py:105\u001b[0m, in \u001b[0;36mget_file_activations\u001b[0;34m(files, model, batch_size, dims, device, num_workers)\u001b[0m\n\u001b[1;32m    102\u001b[0m     batch_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(files)\n\u001b[1;32m    104\u001b[0m dataset \u001b[39m=\u001b[39m ImagePathDataset(files)\n\u001b[0;32m--> 105\u001b[0m dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataLoader(\n\u001b[1;32m    106\u001b[0m     dataset,\n\u001b[1;32m    107\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    108\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    109\u001b[0m     drop_last\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    110\u001b[0m     num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    113\u001b[0m pred_arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((\u001b[39mlen\u001b[39m(files), dims))\n\u001b[1;32m    115\u001b[0m start_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:283\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    279\u001b[0m             sampler \u001b[39m=\u001b[39m SequentialSampler(dataset)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[39mif\u001b[39;00m batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m batch_sampler \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     \u001b[39m# auto_collation without custom batch_sampler\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     batch_sampler \u001b[39m=\u001b[39m BatchSampler(sampler, batch_size, drop_last)\n\u001b[1;32m    285\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n\u001b[1;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_last \u001b[39m=\u001b[39m drop_last\n",
      "File \u001b[0;32m~/.virtualenvs/torch/lib/python3.8/site-packages/torch/utils/data/sampler.py:215\u001b[0m, in \u001b[0;36mBatchSampler.__init__\u001b[0;34m(self, sampler, batch_size, drop_last)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, sampler: Union[Sampler[\u001b[39mint\u001b[39m], Iterable[\u001b[39mint\u001b[39m]], batch_size: \u001b[39mint\u001b[39m, drop_last: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     \u001b[39m# Since collections.abc.Iterable does not check for `__getitem__`, which\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[39m# is one way for an object to be an iterable, we don't do an `isinstance`\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[39m# check here.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_size, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_size, \u001b[39mbool\u001b[39m) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m    214\u001b[0m             batch_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbatch_size should be a positive integer value, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mbut got batch_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(batch_size))\n\u001b[1;32m    217\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(drop_last, \u001b[39mbool\u001b[39m):\n\u001b[1;32m    218\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdrop_last should be a boolean value, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mdrop_last=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(drop_last))\n",
      "\u001b[0;31mValueError\u001b[0m: batch_size should be a positive integer value, but got batch_size=0"
     ]
    }
   ],
   "source": [
    "m1, s1 = compute_statistics_of_path(dataset_path, inception_model, BATCH_SIZE, 2048, device, 12)\n",
    "print(m1, s1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/art_dataset/resized/resized/inception_statistics.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/elias/dev/gan_project/src/compute_fid.ipynb Cell 7'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhome_desktop/home/elias/dev/gan_project/src/compute_fid.ipynb#ch0000006vscode-remote?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39;49msavez(dataset_statistics_path, m\u001b[39m=\u001b[39;49mm1, s\u001b[39m=\u001b[39;49ms1)\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msavez\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py:618\u001b[0m, in \u001b[0;36msavez\u001b[0;34m(file, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=537'>538</a>\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_savez_dispatcher)\n\u001b[1;32m    <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=538'>539</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msavez\u001b[39m(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m    <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=539'>540</a>\u001b[0m     \u001b[39m\"\"\"Save several arrays into a single file in uncompressed ``.npz`` format.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=540'>541</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=541'>542</a>\u001b[0m \u001b[39m    Provide arrays as keyword arguments to store them under the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=615'>616</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=616'>617</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=617'>618</a>\u001b[0m     _savez(file, args, kwds, \u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py:715\u001b[0m, in \u001b[0;36m_savez\u001b[0;34m(file, args, kwds, compress, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=711'>712</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=712'>713</a>\u001b[0m     compression \u001b[39m=\u001b[39m zipfile\u001b[39m.\u001b[39mZIP_STORED\n\u001b[0;32m--> <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=714'>715</a>\u001b[0m zipf \u001b[39m=\u001b[39m zipfile_factory(file, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m\"\u001b[39;49m, compression\u001b[39m=\u001b[39;49mcompression)\n\u001b[1;32m    <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=716'>717</a>\u001b[0m \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m namedict\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=717'>718</a>\u001b[0m     fname \u001b[39m=\u001b[39m key \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py:112\u001b[0m, in \u001b[0;36mzipfile_factory\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=109'>110</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mzipfile\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=110'>111</a>\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39mallowZip64\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/elias/.virtualenvs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py?line=111'>112</a>\u001b[0m \u001b[39mreturn\u001b[39;00m zipfile\u001b[39m.\u001b[39;49mZipFile(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/lib/python3.9/zipfile.py:1239\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/zipfile.py?line=1236'>1237</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/zipfile.py?line=1237'>1238</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///usr/lib/python3.9/zipfile.py?line=1238'>1239</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mopen(file, filemode)\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/zipfile.py?line=1239'>1240</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/zipfile.py?line=1240'>1241</a>\u001b[0m         \u001b[39mif\u001b[39;00m filemode \u001b[39min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/art_dataset/resized/resized/inception_statistics.npz'"
     ]
    }
   ],
   "source": [
    "\n",
    "np.savez(dataset_statistics_path, m=m1, s=s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n",
      "(2048, 2048)\n"
     ]
    }
   ],
   "source": [
    "dataset_statistics_file = np.load(dataset_statistics_path)\n",
    "m = dataset_statistics_file[\"m\"]\n",
    "s = dataset_statistics_file[\"s\"]\n",
    "print(m.shape)\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute statistics for batch of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 128, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(BATCH_SIZE, latent_dim, 1, 1).to(device)\n",
    "generated_images = model(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = inception_model(generated_images)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_pred = pred.squeeze().detach().cpu().numpy()\n",
    "mu2 = np.mean(resized_pred, axis=0)\n",
    "sigma = np.cov(resized_pred, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n",
      "(2048, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(mu2.shape)\n",
    "print(sigma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545.1638588031259"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_frechet_distance(mu2, sigma, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/elias/dev/personal/art-gan-project/data/celeba/images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 46/33767 [00:13<2:53:53,  3.23it/s]"
     ]
    }
   ],
   "source": [
    "from fid import FID\n",
    "ds_fid = FID(\"celeba\", IMAGE_SIZE, BATCH_SIZE)\n",
    "\n",
    "ds_fid.compute_statistics_of_path()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64df8562ad1e0b9a54ae19b6f8c3a592180281d35bf5de73bc27ee58f75114dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
